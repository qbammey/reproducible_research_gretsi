---
title: "Reproducible Research: From Code to Career"
subtitle: "How to save time, build trust, and maximize your impact"
author: "Adrien Kr√§henb√ºhl (ICube, Universit√© de Strasbourg), Quentin Bammey (Image and Visual Representation Lab, √âcole Polytechnique F√©d√©rale de Lausanne), Gabriele Facciolo (Centre Borelli, ENS Paris-Saclay)"
lightbox: true
scrollable: true
format: 
  revealjs:
    theme: style/theme.scss
    embed-resources: false
    view-distance: 800
    mermaid: {}
---

## Title Slide {.title-slide}

QR CODE HERE TBD

::: footer
Get all the code from this session.
:::

::: notes
Good morning everyone, and welcome. My goal over the next hour is to give you a set of practical tools that can fundamentally change the impact of your research. But first, I want to start with a quick show of hands...
:::


## A new SOTA has been established!

![](images/breakthrough.png)


---

## The Researcher's Journey: A Case Study

Let's walk through a realistic attempt to run the code from a newly published paper.

::: {.r-stack}

::: {.fragment}
````{=html}
<div class="panel-code">
<pre><code>$ python train.py

Traceback (most recent call last):
  File "train.py", line 5, in <module>
    from fancy_layers import CustomAttentionBlock
<span style="color: #d83939; font-weight: bold;">ModuleNotFoundError: No module named 'fancy_layers'</span></code></pre>
</div>
````
:::

::: {.fragment}
````{=html}
<div class="panel-code">
<pre><code>$ python train.py --fix_dependencies

Running feature extraction...
  File "train.py", line 73, in <module>
    features = extract_features(data)
  File "/app/utils/features.py", line 31, in extract_features
    fd, hog_image = hog(image, orientations=8, visualise=True)
<span style="color: #d83939; font-weight: bold;">TypeError: hog() got an unexpected keyword argument 'visualise'</span></code></pre>
</div>
````
:::

::: {.fragment}
````{=html}
<div class="panel-code">
<pre><code>$ python train.py --fix_dependencies --legacy_skimage --fix_paths

Model training started...
Data loaded successfully.
Epoch 1/100
[...]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 2854, in linear
    return torch.addmm(bias, input, weight.t())
<span style="color: #d83939; font-weight: bold;">RuntimeError: CUDA out of memory. Tried to allocate 12.58 GiB (GPU 0; 15.90 GiB total capacity)</span></code></pre>
</div>
````
:::

::: {.fragment}
````{=html}
<div class="panel-code">
<pre><code>$ python train.py --fix_dependencies --legacy_skimage

Model training started...
Loading data...
Traceback (most recent call last):
  File "train.py", line 150, in <module>
    dataset = H5Dataset(config['dataset_path'])
  File "/app/utils/data.py", line 17, in __init__
    self.h5_file = h5py.File(path, 'r')
<span style="color: #d83939; font-weight: bold;">FileNotFoundError: [Errno 2] No such file or directory: '/home/clara/data/grestsi_dataset/train_set.h5'</span></code></pre>
</div>
````
:::


:::




## Our Collective Technical Debt


!["This is fine"](https://i.imgur.com/c4jt321.png){fig-align="center" width="400"}


* The magic parameter `alpha = 0.87` (source: unknown).
* "It worked yesterday, I swear."
* `requirements.txt`: `numpy`, `pytorch` (Reality: 58 hidden packages).
* The true hero of every project: `Final_Final_v2_really_final.ipynb`.

::: notes
And let's be honest, we're not just victims here‚Äîwe're also the culprits. We rush to meet deadlines. We tell ourselves we'll clean up the code and document it later. We create this technical debt. But that debt has consequences, not just for others, but for our own future success. To make this concrete, let me tell you a quick story.
:::


## A Tale of Two Researchers

:::: {.columns}

::: {.column width="50%"}
### Clara, The Sprinter

::: {.fragment}
![](images/clara.png){fig-align="center" width="250"}

- **Focus:** Get the result. Publish the paper. Move on.
- **Code:** "It works on my machine." Pushed to GitHub and forgotten.
:::
:::


::: {.column width="50%"}
### Maria, The Builder

::: {.fragment}
![](images/maria.png){fig-align="center" width="250"}

- **Focus:** Get the result *and* make it usable.
- **Code:** Packaged with a clean, well-described, usable implementation.
:::
:::
::::


::: notes
Meet Clara and Maria. Both are brilliant researchers. They both developed fantastic new methods and got their papers accepted right here at GRETSI. They had similar results, but they had very different philosophies about their code. Let's fast forward one year and see how their choices played out.
:::

---

## One Year Later: Clara's Path
### The Sprinter's Journey

```{mermaid}
flowchart TD
    A[Paper Published] --> B{Industry Contact};
    B --> C[2 weeks later: code fails];
    C --> D[Opportunity lost];
    A --> E{Journal Extension};
    E --> F[3 Weeks to reproduce own results];
    F --> G[Time lost];
    A --> H{GitHub Repo};
    H --> I[3 ‚ÄúDoesn't work‚Äù issues];
    I --> J[Impact Limited];
    classDef default color:#ddddee, fill:#1d1d2a, border:#ddddee;
```

::: notes
Clara sprinted to the deadline. But her 'finished' paper was built on a fragile foundation. When a potential collaborator or employer came knocking, the door was effectively closed. Even worse, her most important collaborator‚Äî'Future Clara'‚Äîwas stuck, wasting weeks rebuilding what she had already done. The initial splash of the publication quickly faded.
:::


## One Year Later: Maria's Path
### The Builder's Journey

```{mermaid}
flowchart TD
    A[Paper Published] --> B{Industry Contact};
    B --> C[Demo & Container work instantly];
    C --> D[Collaboration Gained];
    A --> E{Community};
    E --> F[Code used as baseline];
    F --> G[Reputation Built];
    A --> H{GitHub Repo};
    H --> I[150 Stars, Pull Requests];
    I --> J[Impact Multiplied];
    classDef default color:#ddddee, fill:#1d1d2a, border:#ddddee;
```

::: notes
Maria, on the other hand, invested a little extra time‚Äîmaybe 10%‚Äîto build something robust. This small, upfront investment acted as an impact multiplier. Her work wasn't just *read*; it was *used*, *verified*, and *built upon*. She didn't just publish a paper; she delivered a reliable tool to the community, and the community rewarded her for it.
:::

---

## The Core Principle

::: {.fragment .fade-in-then-semi-out}
$$Impact = Performance \times Usability$$
:::

::: {.fragment}
A brilliant result that no one can use has an impact of zero.
:::

::: {.fragment}
**Reproducibility turns your paper from a claim into a contribution.**
:::

::: notes
The difference between Clara and Maria comes down to this single principle. For too long, our community has been obsessed with the 'Performance' term. But the true, lasting impact of our work is a product of its performance *and* its usability. The story of Clara and Maria shows us that reproducibility isn't just an ethical nice-to-have; it's a core driver of scientific and career success.
:::

---

## Our Goal Today: The Builder's Toolkit

1.  **The Framework:** Define the pillars of reproducibility.
2.  **The Toolbox:** Go from a messy script to a clean, usable and understandable implementation.
3.  **The Payoff:** Create and deploy an interactive web demo of your code in minutes.

::: {.fragment}
### You will leave here with a concrete plan to become a 'Builder'.
:::

::: notes
The good news is that everything Maria did is accessible to all of us. It doesn't require being a software engineering expert. It just requires knowing a few key tools and a simple workflow. That is our mission for the rest of this session. We'll establish a clear framework, I'll walk you through the essential tools in a live demo, and we'll see the payoff by building an interactive demo. By the end, you'll have a practical plan to ensure your next great idea has the impact it deserves. Let's get started.
:::

# The pillars of reproducibility
## The Pillars of Reproducibility

To understand these concepts, let's run a simple experiment: can a small neural network tell the difference between a T-shirt and a pair of Trousers?

---

### Pillar 1: Repeatability

::: {.callout-note}
#### Repeatability: "Can I get the exact same result twice?"
This is the baseline: achieving bit-for-bit identical results by fixing every variable, especially random seeds.
:::

The key is fixing the `random_state`. Let's run the *exact same code* twice, with the seed locked to `42`.

:::: {.columns}

::: {.column width="60%"}
```{python}
#| echo: true
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
import plotly.graph_objects as go

# Function to run one complete experiment
def run_experiment(seed):
    # Load the Fashion-MNIST dataset
    X, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True, as_frame=False, parser='liac-arff')

    # Filter for two classes: 'T-shirt' (0) and 'Trouser' (1)
    tshirt_trouser_indices = np.where((y == '0') | (y == '1'))
    X_subset, y_subset = X[tshirt_trouser_indices], y[tshirt_trouser_indices]
    
    # Split the data with a fixed random_state
    X_train, X_test, y_train, y_test = train_test_split(
        X_subset, y_subset, test_size=0.2, random_state=seed
    )

    # Define and train a model with a fixed random_state
    model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, random_state=seed)
    model.fit(X_train, y_train)
    
    # Evaluate and return the accuracy
    predictions = model.predict(X_test)
    return accuracy_score(y_test, predictions)

# Run the experiment twice with the same seed
seed = 42
accuracy_run_1 = run_experiment(seed)
accuracy_run_2 = run_experiment(seed)

print(f"Seed used for both runs: {seed}")
print(f"Accuracy of Run 1: {accuracy_run_1:.4f}")
print(f"Accuracy of Run 2: {accuracy_run_2:.4f}")
```
:::

::: {.column width="40%"}
```{python}
#| echo: false
fig = go.Figure([
    go.Bar(name='Run 1', x=['Accuracy'], y=[accuracy_run_1]),
    go.Bar(name='Run 2', x=['Accuracy'], y=[accuracy_run_2])
])
fig.update_layout(
    title='Repeatability: Two Runs, Same Seed',
    yaxis_title='Accuracy',
    yaxis=dict(range=[0.95, 1.0]),
    barmode='group',
    legend_title='Experiment Run',
    template='plotly_dark'
)
fig.show()
```
Perfect match. This is **repeatability**.
:::
::::

---

### Pillar 2: Reproducibility

::: {.callout-note}
#### Reproducibility: "Does my finding hold if we change the random seed?"
The goal is to reach the same scientific conclusion, even if the numbers aren't identical.
:::

We'll run the same experiment five times, each with a *different* random seed, to check the stability of our result.

:::: {.columns}
::: {.column width="60%"}
```{python}
#| echo: true
# We re-use the run_experiment function from the previous slide.

# Run the experiment 5 times with different seeds
seeds = [0, 42, 101, 1337, 2025]
accuracies = []
for seed in seeds:
    acc = run_experiment(seed)
    accuracies.append(acc)
    print(f"Seed: {seed:<4} -> Accuracy: {acc:.4f}")
```
:::
::: {.column width="40%"}
```{python}
#| echo: false
fig = go.Figure(data=go.Box(
    y=accuracies,
    name='Model Accuracy',
    boxpoints='all',
    jitter=0.3,
    pointpos=-1.8
))

fig.update_layout(
    title='Reproducibility: Accuracy Across 5 Seeds',
    yaxis_title='Accuracy',
    yaxis=dict(range=[0.95, 1.0]),
    template='plotly_dark'
)
fig.show()

```
The numbers wiggle, but the story is the same: accuracy is ~99%. This is **reproducibility**.
:::
::::

---

### Pillar 3: Replicability

::: {.callout-note}
#### Replicability: "Is my finding a real phenomenon, or a quirk of my dataset?"
The gold standard: testing if a scientific discovery holds up under a new, independent study.
:::

Let's challenge our finding by applying the *same model* to a new task: telling 'Sandals' from 'Ankle Boots'.

:::: {.columns}
::: {.column width="60%"}
```{python}
#| echo: true

def run_replication_experiment(seed, class1_label, class2_label):
    # Load the same Fashion-MNIST dataset
    X, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True, as_frame=False, parser='liac-arff')

    # Filter for two NEW classes: 'Sandal' (5) and 'Ankle Boot' (9)
    sandal_boot_indices = np.where((y == class1_label) | (y == class2_label))
    X_subset, y_subset = X[sandal_boot_indices], y[sandal_boot_indices]
    
    # Split, train, and evaluate using the same model architecture
    X_train, X_test, y_train, y_test = train_test_split(
        X_subset, y_subset, test_size=0.2, random_state=seed
    )
    model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, random_state=seed)
    model.fit(X_train, y_train)
    
    return accuracy_score(model.predict(X_test), y_test)
# Run the replication experiment
accuracy_replication = run_replication_experiment(seed=42, class1_label='5', class2_label='9')

# For comparison, let's use our first result
accuracy_original = accuracy_run_1 
 
#print(f"Original Task (T-shirt vs Trouser) Accuracy: {accuracy_original:.4f}")
print(f"Replication Task (Sandal vs Boot) Accuracy: {accuracy_replication:.4f}")
```
:::

::: {.column width="40%"}
```{python}
#| echo: false
fig = go.Figure([
    go.Bar(name='Original Study', x=['T-shirt vs Trouser'], y=[accuracy_original]),
    go.Bar(name='Replication Study', x=['Sandal vs Ankle Boot'], y=[accuracy_replication])
])
fig.update_layout(
    title='Replicability: Testing the Finding on New Data',
    yaxis_title='Accuracy',
    yaxis=dict(range=[0.9, 1.0]),
    template='plotly_dark'
)
fig.show()
```
The core discovery holds up under new scrutiny. This is the gold standard: **replicability**.
:::
::::



---

## Beyond Running Code: The Paper as a Blueprint

::: {.columns}

::: {.column width="50%"}
![A clean architectural blueprint representing a scientific paper.](images/blueprint.png)
:::

::: {.column width="50%"}
![A complex building, based on the blueprint but with messy, undocumented features, representing the actual code.](images/blueprint_result.png)
:::

:::

> So far, we've focused on re-running existing code. Now, we ask the deeper questions:
>
> 1.  If your code vanished, could someone rebuild it from your paper alone? (**Longevity**)
> 2.  Does your code contain "secret features" not shown in the blueprint? (**Validation**)

---

## The Case of the Vague Paper: A Rogues' Gallery {.smaller}

:::: {.columns}
::: {.column width="25%"}
![The Magic Number](images/mugshot_number.png)

**Crime:** Unexplained constants critical to performance.
*Example: `learning_rate = 0.00137`.*
:::
::: {.column width="25%"}
![The Vague Preprocessing](images/mugshot_image.png)

**Crime:** Hand-wavy descriptions of data preparation.
*Example: "...images were normalized..."*
:::


::: {.column width="25%"}
![The Undocumented Trick](images/mugshot_rabbit.png)

**Crime:** Crucial implementation details absent from the paper.
*Example: Using gradient clipping or a specific weight initialization, filtering out results in saturated regions of an image.*
:::
::: {.column width="25%"}
![The Vague data](images/mugshot_film.png)

**Crime:** Vague description of the training/evaluation data.
*Example: Using unreferenced datasets, vague description of the data that was acquired for this paper.*
:::
::::

---

## A Checklist for Clarity

A paper that perfectly describes its method is resilient and trustworthy. Here‚Äôs how to write one.

- [ ] **Justify constants:** If a number isn't obvious, explain its origin (e.g., "from an empirical sweep," "as in Smith et al., 2025").

- [ ] **Document all steps and pre/post-processing:** Give the equation for normalization (`(x - mean) / std`) and the exact values used. Name the filter and its parameters. Publish a hyperparameter table.

- [ ] **Use Pseudocode:** For any complex algorithm or training loop, provide clear pseudocode. It is the ultimate bridge between theory and code.

- [ ] **Specify data**: Explicitly describe which data you use, in which way. If you acquire data for the paper, follow reproducibility practices for data.


##  A Checklist for Clarity
::: {.callout-tip icon="true"}
## The Gold Standard

Journals like **IPOL (Image Processing On Line)** are built on this principle. The implementation and article are peer-reviewed side-by-side to ensure the article details the method well-enough that it could be re-implemented from scratch. This guarantees the method's preservation and validity.
:::
<embed src="ipol_article.pdf" width="100%" height="800" type="application/pdf">

### Demo
##  A Checklist for Clarity
::: {.callout-tip icon="true"}
## The Gold Standard

Journals like **IPOL (Image Processing On Line)** are built on this principle. The implementation and article are peer-reviewed side-by-side to ensure the article details the method well-enough that it could be re-implemented from scratch. This guarantees the method's preservation and validity.
:::
<iframe src="https://ipolcore.ipol.im/demo/clientApp/demo.html?id=420&archive=791228" width="100%" height="800px" style="border:none; display:block; margin:0 auto; background-color:white"></iframe>


# From zero to Reproducible

## Starting Point: The Research Notebook

:::: {.columns}
::: {.column width="60%"}
- ‚úÖ Rapid exploration & prototyping
- ‚úÖ Easy visualization
- ‚ùå Non-linear execution hides the true workflow
- ‚ùå "Hidden state" makes results hard to trust
- ‚ùå Impossible to use as a simple tool

### How do we turn this sketch into a reliable tool?
:::
::: {.column width="40%"}
[Prompt: A screenshot of a very long and messy Jupyter Notebook with out-of-order cell execution numbers.](images/messy_notebook.png)
:::
::::

---

## Level 1: Scripts & A Blueprint

The first step is to create a clear, reusable structure.

:::: {.columns}
::: {.column width="30%"}

![Blueprint](images/diagram_blueprint.svg){fig-align="center"}
:::
::: {.column width="70%"}
### Checklist for a Good Structure
- **Clear instructions:** A `README.md` explaining setup and execution.
- Separate scripts to retrain the model, reproduce benchmark results, and use the model.
- **Provide pre-trained Models:** Don't force users to retrain everything.
:::
::::
---

## Level 2: The Basic Parts List

Your code can't run in a vacuum. You must specify all its dependencies with **pinned versions**.

:::: {.columns}
::: {.column width="28%"}
**‚ùå**
```txt
# requirements.txt
numpy
torch
```
:::
::: {.column width="28%"}
**‚ùå**
```txt
# requirements.txt
numpy
torch
tqdm
pillow
imageio
```
:::
::: {.column width="39%"}
**‚úÖ**
```txt
# requirements.txt
numpy==1.26.4
torch==2.4.1
tqdm==4.4.1
pillow=11.3.0
imageio==2.37.0
```
:::
::::

**But:** a `requirements.txt` doesn't lock the dependencies *of your dependencies*. Results can still vary.

---

## Level 3: The Complete Bill of Materials

::: {.incremental}
- **Solution:** Lock the *entire* environment with modern tools.
- **Our Choice:** **`uv`**.
- **Why?** It's incredibly fast, all-in-one (installer, venv manager), and creates a perfect, reproducible lock file.
:::

---

## Hands-On: A Perfect Python Environment

::: {.columns}
::: {.column width="50%"}
#### 1. For the Author

```bash
# Initialize a new project
$ uv init

# Pin a Python version
$ uv python pin 3.11

# Add packages to pyproject.toml
# and generate a uv.lock file
$ uv add numpy==1.26.4 torch
```
:::
::: {.column width="50%"}
#### 2. For the User

```bash
# Clone the repository
$ git clone ...

# Install the EXACT environment
# from the uv.lock file
$ uv sync

# Run the code
$ uv run predict.py  # equivalent to uv run python predict.py
$ uv run my_executable
```
:::
:::

Use `uv` from the beginning, even when experimenting and developing!

To use libraries without packaging them in the final environment (e. g. Jupyter notebooks):
```bash
$ uv run --with jupyter --with matplotlib jupyter lab 
```

---

## When Python Isn't Enough...

What about dependencies on the operating system itself?

![The dependency iceberg](images/diagram_iceberg.svg){fig-align="center"}

---

## Level 4: The 'Computer in a File'

**Solution:** A **Container** (like Docker) packages everything:
Code + Python Environment + OS Libraries.

### Hands-On: The `Dockerfile` Recipe
```dockerfile
# Start from a specific Python base image
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install uv
RUN pip install uv

# Copy ONLY the lock files
COPY pyproject.toml uv.lock ./

# Install the exact Python env
RUN uv sync --no-cache

# Copy the rest of your code
COPY . .

# Define a default command
CMD ["uv", "run", "python", "predict.py"]
```
Now anyone with Docker can run your code, identically, *everywhere*.

---

## Summary: Climbing the Ladder

![Ladder](images/diagram_ladder.svg){fig-align="center"}

Even with a perfect container, some challenges remain...

---

## The CUDA Challenge

**The Problem:** A dependency matrix of Hardware ‚Üí Driver ‚Üí Toolkit ‚Üí Library.

::: {.columns}
::: {.column width="50%"}
#### Solution Part 1: Inside the Container
Use NVIDIA's official base images in your `Dockerfile` to lock the CUDA Toolkit.

```dockerfile
FROM nvidia/cuda:12.6.1-cudnn-devel-ubuntu22.04
...
```
:::
::: {.column width="50%"}
#### Solution Part 2: On the Host
The user needs the NVIDIA Container Toolkit and a sufficiently new driver.

**Your Job:** Be explicit! State the required driver version in your `README.md`.
:::
:::

---

## Remaining Obstacles: Data & Hardware

::: {.columns}
::: {.column width="50%"}
### The Data Barrier
- **Size:** Provide download scripts; use DVC.
- **Access:** Offer a "demo mode" with public/synthetic data.
- **Format:** Document precisely and provide conversion scripts.
:::
::: {.column width="50%"}
### The Hardware Chasm
- **CPU (x86/ARM):** Provide multi-arch Docker builds.
- **RAM/Resources:** Document minimum requirements clearly.
:::
:::

---

## Remaining Obstacles: Software Dependencies

::: {.columns}
::: {.column width="50%"}
#### The Polyglot Project
Your code might mix Python, C++, Rust, other libraries...

[TBD Logos of languages](images/polyglot_logos.png)
:::
::: {.column width="50%"}
#### The Solution
- **Containerization is essential.** Your `Dockerfile` must install *all* required runtimes.
- **Be transparent** about paid licenses (e.g., MATLAB) in your `README`.
:::
:::

---

## The Final Mile: Usability

Even if code is runnable, is it *usable*?

::: {.panel-tabset}

### Bad CLI

**Problem:** A command line with dozens of obscure, undocumented flags is hostile to users.
```bash
# What do these even mean?!
$ python run.py -a 0.1 -b 32 --z_dim 128 -fe_lr 0.00137
```
**Solution:** Use libraries like `Typer` or `Argparse` to create a clean, self-documenting CLI with help text.

### No Samples

**Problem:** A user clones your repository but has no correctly formatted data to test the `predict.py` script.

**Solution:** Always include a `sample_data/` directory and a `run_demo.sh` script. A "Quick Start" that runs in seconds is invaluable.

### No UI/Demo

**Problem:** The command line is still a major barrier for many potential users (collaborators, industry, other fields).

**Solution:** Wrapping your script in an online demo makes it universally accessible and dramatically increases its impact.

:::

# Creating a demo

## Why Bother Making a Demo?

A demo transforms your research from a static paper into a living tool.

::: {.columns}
::: {.column width="33%"}
#### üëÅÔ∏è
#### Visibility & Impact
Go beyond the PDF. A demo makes your work discoverable and understandable to a much wider audience.
:::
::: {.column width="33%"}
#### üí¨
#### Usability & Feedback
Let people *use* your research. It's the fastest way to get feedback, find bugs, and spark collaborations.
:::
::: {.column width="33%"}
#### ‚úÖ
#### Proof of Work
A working demo is the ultimate proof that your code isn't just theory. It runs, it works, and it builds trust.
:::
:::

---

## Create a demo with Streamlit

![Streamlit](images/streamlit_logo.svg){width=100}

- **What is it?** An open-source Python library to build and share web apps for machine learning and data science.

- **Why Streamlit for this tutorial?**
  - It's just a Python script. No HTML/CSS/JS required.
  - It turns data scripts into shareable apps in minutes.
  - The development workflow is fast and intuitive.

---

## Example 1: A Minimal Demo

Here's how to wrap a function in a simple user interface.

  [Live Demo](https://qbammey-r-example-demoexamplesoutlier-colour-transferapp-ymhhsf.streamlit.app)


```python
# app.py
import io
import numpy as np
import streamlit as st
from PIL import Image
import run  # your run.py in the same folder

st.set_page_config(page_title="OT Color Transfer (tutorial)", layout="centered")
st.title("üé® OT Color Transfer (tutorial)")

# uploads
base_file = st.file_uploader("Base image (to stylize)", type=["jpg", "jpeg", "png"])
target_file = st.file_uploader("Target image (style source)", type=["jpg", "jpeg", "png"])

# parameters (defaults match run.py)
col1, col2 = st.columns(2)
with col1:
    downscale = st.number_input("Downscale size", min_value=8, max_value=256, value=50, step=1)
    metric = st.selectbox("Metric", ["l2", "l1"], index=0)
with col2:
    reg = st.number_input("Entropic reg (0 = EMD)", min_value=0.0, value=0.0, step=0.01, format="%.4f")
    sinkhorn_max_iter = st.number_input("Sinkhorn max iters", min_value=100, max_value=20000, value=1000, step=100)

run_btn = st.button("Run")

if run_btn:
    if base_file is None or target_file is None:
        st.warning("Please upload both images.")
    else:
        # load to [0,1] float RGB
        base_img = np.asarray(Image.open(base_file).convert("RGB"), dtype=np.float64) / 255.0
        target_img = np.asarray(Image.open(target_file).convert("RGB"), dtype=np.float64) / 255.0

        styl = run.color_transfer_ot(
            base_img=base_img,
            target_img=target_img,
            downscale=int(downscale),
            metric=metric,
            reg=float(reg),
            sinkhorn_max_iter=int(sinkhorn_max_iter),
        )

        st.subheader("Result")
        st.image(styl, use_container_width=True)

        # quick download
        buf = io.BytesIO()
        Image.fromarray((np.clip(styl, 0, 1) * 255).astype(np.uint8)).save(buf, format="PNG")
        st.download_button("Download PNG", data=buf.getvalue(), file_name="stylized.png", mime="image/png")


```





---

## Deploying Your Demo

Locally : `uv run streamlit run app.py` (or, without `uv`, `streamlit run app.py`)

Making your app public is also easy with Streamlit Community Cloud (free).

```{.mermaid}
flowchart LR
    A["<img src='[https://cdn.iconscout.com/icon/free/png-256/free-github-153-675523.png](https://cdn.iconscout.com/icon/free/png-256/free-github-153-675523.png)' width='50' /><br>1. Push to GitHub"]
    B["<img src='[https://streamlit.io/images/brand/logo-mark-color.png](https://streamlit.io/images/brand/logo-mark-color.png)' width='50' /><br>2. Connect to Streamlit Cloud"]
    C["<img src='[https://cdn.iconscout.com/icon/free/png-256/free-link-133-461719.png](https://cdn.iconscout.com/icon/free/png-256/free-link-133-461719.png)' width='50' /><br>3. Get a Public URL!"]
    
    A --> B --> C
```

**Key Steps:**
1.  Put your app, `pyproject.toml`, `uv.lock`, and a `packages.txt` file in a public GitHub repo.
2.  In Streamlit Cloud, link your repo.
3.  Point to your main file path (e.g., `demo/app.py`).
4.  Click Deploy!

---

## How Far Can You Go? A Polished App

With a bit more code, a simple demo can become a polished application with caching, state management, and a better layout.

[Live demo](https://example-demoexamplesoutlier-colou-dq04w1.streamlit.app/)



```python
import io
import numpy as np
import streamlit as st
from PIL import Image
from pathlib import Path

# --- Import your research function ---
import run

# --- Define the directory of the current script to find example images ---
# This is the key to making file paths robust. `Path(__file__)` gets the path
# to this .py file, and `.parent` gets the directory it's in.
SCRIPT_DIR = Path(__file__).parent

# --- Page Configuration ---
st.set_page_config(
    page_title="OT Color Transfer (Advanced)",
    layout="wide"
)

# --- Caching the Core Function ---
@st.cache_data
def cached_color_transfer(base_img_bytes, target_img_bytes, downscale, metric, reg, sinkhorn_max_iter):
    """
    A wrapper around the core function to make it cacheable.
    We pass image bytes instead of numpy arrays because arrays are not hashable.
    """
    base_img = np.asarray(Image.open(io.BytesIO(base_img_bytes)).convert("RGB"), dtype=np.float64) / 255.0
    target_img = np.asarray(Image.open(io.BytesIO(target_img_bytes)).convert("RGB"), dtype=np.float64) / 255.0
    
    return run.color_transfer_ot(
        base_img=base_img,
        target_img=target_img,
        downscale=int(downscale),
        metric=metric,
        reg=float(reg),
        sinkhorn_max_iter=int(sinkhorn_max_iter),
    )

# --- Helper function to load example images using pathlib ---
def load_example_image(image_filename):
    """Loads an image from a path relative to the script's directory."""
    # Construct the full, absolute path to the image
    image_path = SCRIPT_DIR / image_filename
    with open(image_path, "rb") as f:
        return f.read()

# --- Main App UI ---
st.title("üé® Advanced OT Color Transfer")
st.write("This demo showcases a more robust Streamlit app with caching, state management, and error handling.")

# --- Sidebar for Inputs and Parameters ---
with st.sidebar:
    st.header("Inputs")
    
    # 2. Update the example options with your filenames and names
    example_options = {
        "Example 1": ("B2.jpg", "A1.jpg"),
        "Example 2": ("B2.jpg", "C1.jpg"),
    }
    
    source_type = st.radio("Choose image source:", ["Upload your own", "Use an example"], index=1) # Default to example

    if source_type == "Upload your own":
        base_file = st.file_uploader("Base image (to stylize)", type=["jpg", "jpeg", "png"])
        target_file = st.file_uploader("Target image (style source)", type=["jpg", "jpeg", "png"])
    else:
        selected_example = st.selectbox("Select an example:", list(example_options.keys()))
        base_filename, target_filename = example_options[selected_example]
        # Use our robust helper function to load the images
        base_file = load_example_image(base_filename)
        target_file = load_example_image(target_filename)

    st.header("Parameters")
    with st.expander("Algorithm Controls", expanded=True):
        downscale = st.number_input("Downscale size", 8, 256, 50, 1)
        metric = st.selectbox("Metric", ["l2", "l1"], 0)
        reg = st.number_input("Entropic reg (0 = EMD)", 0.0, 1.0, 0.0, 0.01, format="%.4f")
        if reg > 0:
            sinkhorn_max_iter = st.number_input("Sinkhorn max iters", 100, 20000, 1000, 100)
        else:
            sinkhorn_max_iter = 1000

    run_btn = st.button("Run Style Transfer", type="primary")

# --- Main Area for Displaying Images ---
col1, col2 = st.columns(2)
if base_file is not None:
    col1.image(base_file, caption="Base Image", use_container_width=True)
if target_file is not None:
    col2.image(target_file, caption="Target Style", use_container_width=True)
    
if run_btn:
    if base_file is None or target_file is None:
        st.warning("Please provide both a base and a target image.")
    else:
        base_bytes = base_file.getvalue() if hasattr(base_file, 'getvalue') else base_file
        target_bytes = target_file.getvalue() if hasattr(target_file, 'getvalue') else target_file
        
        with st.spinner('Stylizing image... this may take a moment.'):
            try:
                stylized_image = cached_color_transfer(
                    base_bytes, target_bytes, downscale, metric, reg, sinkhorn_max_iter
                )
                st.session_state['last_result'] = stylized_image
            except Exception as e:
                st.error(f"An error occurred during processing: {e}")
                if 'last_result' in st.session_state:
                    del st.session_state['last_result']

if 'last_result' in st.session_state:
    st.divider()
    st.header("Result")
    result_image = st.session_state['last_result']
    st.image(result_image, caption="Stylized Result", use_column_width=True)

    buf = io.BytesIO()
    Image.fromarray((np.clip(result_image, 0, 1) * 255).astype(np.uint8)).save(buf, format="PNG")
    st.download_button(
        "Download Result",
        data=buf.getvalue(),
        file_name="stylized_result.png",
        mime="image/png"
    )


```

---

## Alternatives to Streamlit

Streamlit is great, but other tools might be better for specific needs.

::: {.columns}
::: {.column width="33%"}
![Gradio](images/gradio_logo.svg){width=100}

Simple, but lacks flexibility. Best for pure ML models, Hugging Face spaces.
:::
::: {.column width="33%"}
![Shiny](images/shiny_logo.png){width=150}
More complex, more versatility. Best if some interactivity (partial reruns) is needed.

:::
::: {.column width="33%"}
![Plotly Logo](images/plotly_logo.png){width=150}

Even more complex, excellent versatility within the Plotly environment.
:::
:::


---

## The Academic Gold Standard: IPOL Demos

::: {.callout-note}
### [Your content on IPOL]
- IPOL (Image Processing On Line) represents a different paradigm...
- Here, the demo is integrated with the peer-reviewed publication itself.
- The focus is on method reproducibility, where the article contains every detail needed for re-implementation...
- To be continued
:::

---
